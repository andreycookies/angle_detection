import datetime
import json
from collections import deque
import torch
import cv2
import tkinter as tk
from tkinter import ttk, filedialog, messagebox
from PIL import Image, ImageTk
import threading
import queue
import time
from ultralytics import YOLO
import numpy as np
import math
from homo import mask_preprocessing, has_no_ones, find_4_points, detect_sheet_angle_no_homography


class VideoProcessorApp:
    def __init__(self, root):
        self.root = root
        self.root.title("YOLO Video Processor")
        self.root.geometry("1200x800")

        self.video_source = None
        self.video_path = None
        self.rtsp_url = None
        self.cap = None
        self.is_playing = False
        self.current_frame = None
        self.handled_model = None
        self.raw_model = None
        self.processing_queue = queue.Queue(maxsize=1)
        self.display_queue = queue.Queue(maxsize=1)
        self.new_video = []
        self.display_fps_5s = 0.0
        self.inference_times = deque(maxlen=30)  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ inference
        
        # üî• GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üöÄ Using device: {self.device}")
        
        if torch.cuda.is_available():
            print(f"   GPU: {torch.cuda.get_device_name(0)}")
            print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
            torch.backends.cudnn.benchmark = True  # –ê–≤—Ç–æ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è cuDNN
            torch.cuda.empty_cache()
        else:
            print("‚ö†Ô∏è  CUDA not available, using CPU")
        
        json_path = r".\video_homo.json"
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            H = np.array(data["homography_matrix"], dtype=np.float64)
        
        self.H = H
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ YOLO
        self.load_yolo_model()

        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
        self.create_widgets()

    def connect_rtsp_camera(self):
        self.ask_rtsp_url()

    def disconnect_source(self):
        """–û—Ç–∫–ª—é—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        self.stop_playback()

        if self.cap:
            self.cap.release()
            self.cap = None

        self.video_source = None
        self.video_path = None
        self.rtsp_url = None
        self.source_info_label.config(text="No video source connected")
        self.play_button.config(state=tk.DISABLED)
        self.fps_label.config(text="FPS: 0")

        self.video_canvas.delete("all")
        self.stats_text.delete(1.0, tk.END)

    def stop_camera(self):
        """–ü–æ–ª–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–∞–º–µ—Ä—ã"""
        self.stop_processing()
        if self.cap:
            self.cap.release()
            self.cap = None
        self.play_button.config(state=tk.DISABLED)

    def show_settings(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–º–µ—Ä—ã"""
        settings_window = tk.Toplevel(self.root)
        settings_window.title("Camera Settings")
        settings_window.geometry("400x300")

        ttk.Label(settings_window, text="RTSP URL:").pack(pady=(10, 5))
        url_entry = ttk.Entry(settings_window, width=50)
        url_entry.pack(pady=5)
        if self.rtsp_url:
            url_entry.insert(0, self.rtsp_url)

        def save_settings():
            new_url = url_entry.get()
            if new_url and new_url != self.rtsp_url:
                self.rtsp_url = new_url
                if self.is_playing:
                    self.stop_processing()
                if self.cap:
                    self.cap.release()
                self.connect_camera()
            settings_window.destroy()

        ttk.Button(settings_window, text="Save", command=save_settings).pack(pady=10)

    def connect_camera(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∫–∞–º–µ—Ä–µ"""
        if not self.rtsp_url:
            return

        try:
            self.cap = cv2.VideoCapture(self.rtsp_url)
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            self.cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'H264'))

            if self.cap.isOpened():
                ret, frame = self.cap.read()
                if ret:
                    self.play_button.config(state=tk.NORMAL)
                    self.current_frame = frame
                    self.display_frame(frame)
                    self.start_processing()
                else:
                    messagebox.showerror("Error", "Connected but cannot receive frames")
                    self.cap.release()
                    self.cap = None
            else:
                messagebox.showerror("Error", f"Cannot connect to camera:\n{self.rtsp_url}")

        except Exception as e:
            messagebox.showerror("Error", f"Connection failed: {str(e)}")

    def ask_rtsp_url(self):
        """–ó–∞–ø—Ä–æ—Å RTSP URL —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        self.rtsp_url = tk.simpledialog.askstring(
            "RTSP Connection",
            "Enter RTSP URL for Hiwatch camera:\n\n" +
            "Common formats:\n" +
            "rtsp://username:password@IP:554/stream\n" +
            "rtsp://IP:554/user=username_password=password_channel=1_stream=0.sdp\n\n" +
            "Example: rtsp://admin:12345@192.168.1.108:554/Streaming/Channels/101",
            initialvalue="rtsp://admin:12345@192.168.1.108:554/Streaming/Channels/101"
        )

        if self.rtsp_url:
            self.connect_camera()
        else:
            messagebox.showwarning("Warning", "No RTSP URL provided.")

    def load_yolo_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ YOLO —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        try:
            print("üì¶ Loading YOLO models...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
            self.handled_model = YOLO('./models/handled_model.pt')
            self.raw_model = YOLO('./models/raw_model.pt')
            
            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ GPU
            self.handled_model.to(self.device)
            self.raw_model.to(self.device)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ GPU
            if self.device.type == 'cuda':
                first_param = next(self.handled_model.model.parameters())
                print(f"‚úÖ Models loaded on: {first_param.device}")
            
            # üî• –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–µ–π (–ø–µ—Ä–≤—ã–π inference –≤—Å–µ–≥–¥–∞ –º–µ–¥–ª–µ–Ω–Ω—ã–π)
            print("üî• Warming up models (compiling CUDA kernels)...")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–º–µ—Ä—ã –∫—Ä–∞—Ç–Ω—ã–µ 32
            dummy_input = torch.randn(1, 3, 640, 640, device=self.device)
            
            with torch.no_grad():
                _ = self.handled_model.model(dummy_input)
                _ = self.raw_model.model(dummy_input)
            
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø–æ—Å–ª–µ –ø—Ä–æ–≥—Ä–µ–≤–∞
            if self.device.type == 'cuda':
                torch.cuda.synchronize()  # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
                torch.cuda.empty_cache()
                print(f"‚úÖ GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
            
            print("‚úÖ Models ready for inference")
            
        except Exception as e:
            print(f"‚ùå Failed to load YOLO models: {e}")
            import traceback
            traceback.print_exc()
            messagebox.showerror("Error", f"Failed to load YOLO model: {str(e)}")

    def create_widgets(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))

        self.root.columnconfigure(0, weight=1)
        self.root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(1, weight=1)
        main_frame.rowconfigure(1, weight=1)

        source_frame = ttk.LabelFrame(main_frame, text="Video Source", padding="10")
        source_frame.grid(row=0, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))

        source_buttons_frame = ttk.Frame(source_frame)
        source_buttons_frame.pack(fill=tk.X)

        ttk.Button(source_buttons_frame, text="Open Video File",
                   command=self.open_video_file, width=15).pack(side=tk.LEFT, padx=(0, 10))
        ttk.Button(source_buttons_frame, text="Connect RTSP Camera",
                   command=self.connect_rtsp_camera, width=15).pack(side=tk.LEFT, padx=(0, 10))
        ttk.Button(source_buttons_frame, text="Disconnect",
                   command=self.disconnect_source, width=10).pack(side=tk.LEFT)

        self.source_info_label = ttk.Label(source_frame, text="No video source connected")
        self.source_info_label.pack(fill=tk.X, pady=(5, 0))

        control_frame = ttk.LabelFrame(main_frame, text="Playback Control", padding="10")
        control_frame.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))

        control_buttons_frame = ttk.Frame(control_frame)
        control_buttons_frame.pack(fill=tk.X)

        self.play_button = ttk.Button(source_buttons_frame, text="Start",
                                      command=self.toggle_play, width=10)
        self.play_button.pack(side=tk.LEFT, padx=(0, 10))

        ttk.Button(source_buttons_frame, text="Stop",
                   command=self.stop_playback, width=10).pack(side=tk.LEFT, padx=(0, 10))

        self.progress_frame = ttk.Frame(control_buttons_frame)
        self.progress_frame.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(20, 0))

        self.progress_var = tk.DoubleVar()
        self.progress_bar = ttk.Progressbar(self.progress_frame, variable=self.progress_var, maximum=100)
        self.progress_bar.pack(side=tk.LEFT, fill=tk.X, expand=True)

        self.progress_label = ttk.Label(self.progress_frame, text="0%", width=5)
        self.progress_label.pack(side=tk.RIGHT, padx=(5, 0))

        stats_frame = ttk.Frame(control_buttons_frame)
        stats_frame.pack(side=tk.RIGHT)

        self.fps_label = ttk.Label(stats_frame, text="FPS: 0")
        self.fps_label.pack(side=tk.RIGHT, padx=(10, 0))

        settings_frame = ttk.Frame(control_buttons_frame)
        settings_frame.pack(side=tk.RIGHT, padx=(20, 0))

        video_frame = ttk.LabelFrame(main_frame, text="Video Output", padding="5")
        video_frame.grid(row=3, column=0, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        video_frame.columnconfigure(0, weight=1)
        video_frame.rowconfigure(0, weight=1)

        self.video_canvas = tk.Canvas(video_frame, bg='black', width=800, height=600)
        self.video_canvas.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))

        stats_frame = ttk.LabelFrame(main_frame, text="Detection Statistics", padding="5")
        stats_frame.grid(row=2, column=0, columnspan=2, sticky=(tk.W, tk.E))

        self.stats_text = tk.Text(stats_frame, height=6, width=80)
        scrollbar = ttk.Scrollbar(stats_frame, orient="vertical", command=self.stats_text.yview)
        self.stats_text.configure(yscrollcommand=scrollbar.set)
        self.stats_text.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)

        self.processing_thread = None
        self.display_thread = None

    def open_video_file(self):
        """–û—Ç–∫—Ä—ã—Ç–∏–µ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–∞"""
        file_path = filedialog.askopenfilename(
            title="Select Video File",
            filetypes=[
                ("Video files", "*.mp4 *.avi *.mov *.mkv *.wmv *.flv"),
                ("All files", "*.*")
            ]
        )

        if file_path:
            self.disconnect_source()
            self.video_source = 'file'
            self.video_path = file_path

            try:
                self.cap = cv2.VideoCapture(file_path)

                if self.cap.isOpened():
                    fps = self.cap.get(cv2.CAP_PROP_FPS)
                    frame_count = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
                    duration = frame_count / fps if fps > 0 else 0
                    width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                    height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

                    info_text = (f"File: {file_path.split('/')[-1]} | "
                                 f"Size: {width}x{height} | FPS: {fps:.1f} | "
                                 f"Frames: {frame_count} | Duration: {duration:.1f}s")
                    self.source_info_label.config(text=info_text)

                    self.play_button.config(state=tk.NORMAL)
                    self.progress_var.set(0)
                    self.show_first_frame()
                else:
                    messagebox.showerror("Error", "Could not open video file")

            except Exception as e:
                messagebox.showerror("Error", f"Error opening video file: {str(e)}")

    def show_first_frame(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–π –∫–∞–¥—Ä –≤–∏–¥–µ–æ"""
        if self.cap:
            ret, frame = self.cap.read()
            if ret:
                self.current_frame = frame
                self.display_frame(frame)
                self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

    def toggle_play(self):
        """–ó–∞–ø—É—Å–∫/–ø–∞—É–∑–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è"""
        if not self.is_playing:
            self.start_processing()
        else:
            self.stop_processing()

    def start_processing(self):
        if not self.cap:
            return

        self.is_playing = True
        self.play_button.config(text="Pause")

        self.processing_thread = threading.Thread(target=self.process_video, daemon=True)
        self.yolo_thread = threading.Thread(target=self.update_processing, daemon=True)
        self.display_thread = threading.Thread(target=self.update_display, daemon=True)

        self.processing_thread.start()
        self.yolo_thread.start()
        self.display_thread.start()

    def stop_processing(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ"""
        self.is_playing = False
        self.play_button.config(text="Play")

    def stop_video(self):
        """–ü–æ–ª–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–∏–¥–µ–æ"""
        self.stop_processing()
        if self.cap:
            self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
            self.show_first_frame()

    def stop_playback(self):
        """–ü–æ–ª–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è"""
        self.stop_processing()
        if self.cap and self.video_source == 'file':
            self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
            self.show_first_frame()
        self.progress_var.set(0)
        self.progress_label.config(text="0%")

    def process_video(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
        fps = self.cap.get(cv2.CAP_PROP_FPS)
        frame_delay = 1.0 / fps if fps > 0 else 0.033

        frame_times = deque()
        window_sec = 5.0

        while self.is_playing and self.cap:
            loop_start = time.time()

            ret, frame = self.cap.read()
            if not ret:
                self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                continue

            now = time.time()
            frame_times.append(now)

            while frame_times and (now - frame_times[0]) > window_sec:
                frame_times.popleft()

            # –í—ã–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∫–∞–¥—Ä—ã –∏–∑ –æ—á–µ—Ä–µ–¥–∏
            while self.processing_queue.full():
                try:
                    self.processing_queue.get_nowait()
                except queue.Empty:
                    break

            try:
                self.processing_queue.put_nowait(frame)
            except queue.Full:
                pass

            processing_time = time.time() - loop_start
            sleep_time = max(0, frame_delay - processing_time)
            time.sleep(sleep_time)

    # def process_frame_with_yolo(self, frame):
    #     """
    #     –í–µ—Ä—Å–∏—è —á—Ç–æ –≤—ã–¥–∞–µ—Ç 25 —Ñ–ø—Å
    #     üî• –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω:
    #     1) YOLO –Ω–∞ —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –∫–∞–¥—Ä–µ
    #     2) –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ bbox –æ–±—Ä–∞—Ç–Ω–æ
    #     3) Homography –¢–û–õ–¨–ö–û –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
    #     """
    #
    #     if self.handled_model is None:
    #         return frame
    #
    #     t0 = time.time()
    #
    #     try:
    #         # ================================
    #         # 1Ô∏è‚É£ Resize –î–õ–Ø YOLO (–ö–†–ò–¢–ò–ß–ù–û)
    #         # ================================
    #         H0, W0 = frame.shape[:2]
    #         YOLO_SIZE = 960  # 640 / 960 / 1280
    #
    #         scale = YOLO_SIZE / max(H0, W0)
    #         new_w = int(W0 * scale)
    #         new_h = int(H0 * scale)
    #
    #         small = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
    #
    #         # ================================
    #         # 2Ô∏è‚É£ YOLO inference (GPU)
    #         # ================================
    #         tensor = torch.from_numpy(small).to(
    #             self.device, non_blocking=True
    #         ).float().div_(255.0)
    #
    #         tensor = tensor.permute(2, 0, 1).unsqueeze(0)
    #
    #         # padding –¥–æ –∫—Ä–∞—Ç–Ω–æ—Å—Ç–∏ 32
    #         pad_h = (32 - tensor.shape[2] % 32) % 32
    #         pad_w = (32 - tensor.shape[3] % 32) % 32
    #         tensor = torch.nn.functional.pad(tensor, (0, pad_w, 0, pad_h))
    #
    #         with torch.no_grad(), torch.amp.autocast('cuda'):
    #             results = self.handled_model(tensor, verbose=False)
    #
    #         if len(results) == 0 or results[0].boxes is None:
    #             return frame
    #
    #         # ================================
    #         # 3Ô∏è‚É£ –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º bbox –æ–±—Ä–∞—Ç–Ω–æ
    #         # ================================
    #         boxes = results[0].boxes.xyxy.clone()
    #
    #         boxes[:, [0, 2]] *= (W0 / new_w)
    #         boxes[:, [1, 3]] *= (H0 / new_h)
    #
    #         boxes_cpu = boxes.cpu().numpy().astype(int)
    #
    #         # ================================
    #         # 4Ô∏è‚É£ Homography –¢–û–õ–¨–ö–û –¥–ª—è bbox
    #         # ================================
    #         centers = []
    #         for x1, y1, x2, y2 in boxes_cpu:
    #             cx = (x1 + x2) * 0.5
    #             cy = (y1 + y2) * 0.5
    #             centers.append([cx, cy])
    #
    #             # –†–∏—Å—É–µ–º bbox
    #             cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)
    #
    #         if centers:
    #             pts = np.array(centers, dtype=np.float32).reshape(-1, 1, 2)
    #             world_pts = cv2.perspectiveTransform(pts, self.H)
    #
    #             # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–æ—á–∫–∏ –≤ –º–∏—Ä–æ–≤—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö (–ø—Ä–∏–º–µ—Ä)
    #             for (cx, cy), (wx, wy) in zip(centers, world_pts.reshape(-1, 2)):
    #                 cv2.putText(
    #                     frame,
    #                     f"({int(wx)}, {int(wy)})",
    #                     (int(cx), int(cy)),
    #                     cv2.FONT_HERSHEY_SIMPLEX,
    #                     0.6,
    #                     (0, 0, 255),
    #                     2
    #                 )
    #
    #         # ================================
    #         # 5Ô∏è‚É£ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    #         # ================================
    #         infer_time = time.time() - t0
    #         self.inference_times.append(infer_time)
    #         avg_inf = sum(self.inference_times) / len(self.inference_times)
    #
    #         self.update_statistics(
    #             angle=0,
    #             fps=self.display_fps_5s,
    #             inference_time=avg_inf
    #         )
    #
    #         return frame
    #
    #     except Exception as e:
    #         print(f"‚ùå process_frame_with_yolo error: {e}")
    #         import traceback
    #         traceback.print_exc()
    #         return frame

    def process_frame_with_yolo(self, frame):
        if self.handled_model is None or self.raw_model is None:
            return frame

        try:
            image = frame.copy()

            # ===== YOLO INFERENCE (GPU SAFE) =====
            with torch.no_grad(), torch.amp.autocast("cuda"):
                results = self.handled_model(
                    image,
                    imgsz=960,
                    device="cuda",
                    verbose=False
                )

            if not results or results[0].masks is None:
                return image

            masks = results[0].masks.data.detach().cpu().numpy()

            if masks.shape[0] != 1:
                return image

            mask = masks[0]
            binary_mask = (mask > 0).astype(np.uint8)

            # ===== —Ç–≤–æ—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞—Å–∫–∏ =====
            m = mask_preprocessing(binary_mask)

            if has_no_ones(m):
                return image

            h, w = image.shape[:2]
            resized_mask = cv2.resize(m, (w, h), interpolation=cv2.INTER_NEAREST)

            approx4, _ = find_4_points(resized_mask)
            angle = detect_sheet_angle_no_homography(resized_mask)

            self.update_statistics(angle, self.display_fps_5s)

            # ===== RAW MODEL =====
            with torch.no_grad(), torch.amp.autocast("cuda"):
                raw_results = self.raw_model(
                    image,
                    imgsz=960,
                    device="cuda",
                    verbose=False
                )

            if not raw_results or raw_results[0].boxes is None:
                return image

            # ‚ùó –í–ê–ñ–ù–û: clone / copy
            boxes = raw_results[0].boxes.xyxy.detach().cpu().numpy().copy()

            for box in boxes:
                x1, y1, x2, y2 = map(int, box)
                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 5)

            return image

        except Exception as e:
            print(f"‚ùå process_frame_with_yolo error: {e}")
            return frame

    # def update_statistics(self, angle, fps, inference_time, cv_time=0, gpu1_time=0, gpu2_time=0):
    #     text = (f"Display FPS: {fps:.1f}\n"
    #             f"Inference: {inference_time*1000:.1f} ms ({1/inference_time:.1f} FPS)\n"
    #             f"  CV ops: {cv_time*1000:.1f} ms\n"
    #             f"  GPU #1: {gpu1_time*1000:.1f} ms\n"
    #             f"  GPU #2: {gpu2_time*1000:.1f} ms\n"
    #             f"–£–≥–æ–ª –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è: {int(angle)}¬∞")
    #     self.root.after(0, lambda: self._update_stats_display(text))

    def update_statistics(self, angle, fps):
        text = (f"Display FPS: {fps:.1f}\n"
                f"–£–≥–æ–ª –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è: {int(angle)}¬∞")
        self.root.after(0, lambda: self._update_stats_display(text))

    def _update_stats_display(self, text):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        self.stats_text.delete(1.0, tk.END)
        self.stats_text.insert(1.0, text)

    def update_display(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
        while self.is_playing:
            try:
                frame = self.display_queue.get(timeout=0.1)
                self.display_frame(frame)
            except queue.Empty:
                continue

    def display_frame(self, frame):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–¥—Ä–∞ –Ω–∞ Canvas"""
        try:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            h, w = frame_rgb.shape[:2]
            canvas_width = self.video_canvas.winfo_width()
            canvas_height = self.video_canvas.winfo_height()

            if canvas_width <= 1:
                canvas_width = 800
            if canvas_height <= 1:
                canvas_height = 600

            scale_x = canvas_width / w
            scale_y = canvas_height / h
            scale = min(scale_x, scale_y, 1.0)

            new_w = int(w * scale)
            new_h = int(h * scale)

            if new_w != w or new_h != h:
                frame_resized = cv2.resize(frame_rgb, (new_w, new_h))
            else:
                frame_resized = frame_rgb

            pil_image = Image.fromarray(frame_resized)
            tk_image = ImageTk.PhotoImage(pil_image)

            self.video_canvas.delete("all")
            self.video_canvas.create_image(
                canvas_width // 2,
                canvas_height // 2,
                image=tk_image,
                anchor=tk.CENTER
            )

            self.video_canvas.tk_image = tk_image

        except Exception as e:
            print(f"Error displaying frame: {str(e)}")

    def update_processing(self):
        frame_times = deque()
        window = 5.0

        while self.is_playing:
            try:
                frame = self.processing_queue.get(timeout=0.1)
                # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π –∫–∞–¥—Ä
                while not self.processing_queue.empty():
                    frame = self.processing_queue.get_nowait()

            except queue.Empty:
                continue

            processed = self.process_frame_with_yolo(frame)

            now = time.time()
            frame_times.append(now)

            while frame_times and now - frame_times[0] > window:
                frame_times.popleft()

            self.display_fps_5s = len(frame_times) / window

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if not self.display_queue.full():
                self.display_queue.put(processed)
            else:
                try:
                    self.display_queue.get_nowait()
                except:
                    pass
                self.display_queue.put(processed)

    def __del__(self):
        """–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.cap:
            self.cap.release()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()